# Analysis of Bias in U.S. History Textbooks Using BERT

## Installation

Create a Python virtual environment and install the necessary packages.

```bash
pip install -r requirements.txt
```

## Usage

Several sample contexts have been extracted from our U.S. History Textbook dataset to perform further analysis of gender bias using our BERT model (fine-tuned on the textbook dataset). Each of the contexts contains a "gender" word and an "interest" word (related to home, work, and achievement) within a 10-token window.

The contexts have been preprocessed and grouped into three files (pr represents the normalized probability of predicting the correct gender):
- pr_low.txt (29 examples): pr < 0.25 (high confidence, incorrect gender prediction)
- pr_med.txt (8 examples): 0.45 < pr < 0.55 (low confidence)
- pr_high.txt (37 examples): pr > 0.9999 (high confidence, correct gender prediction)

Each input file is formatted as an array of entries: (`tokens_tensor`, `segments_tensor`, `tokenized_text`, (`gender_index`, `query_index`, `gender_word`, `query_word`)). 
- `tokens_tensor`, `segments_tensor`, `tokenized_text`: input context encoded into the format needed for feeding into BERT
- `gender_index`, `gender_word`: gender word and its index in the input context
- `query_index`, `query_word`: interest word and its index in the input context (where interest word is related to home, work, or achievement)

Run the `predict_mask.py` script to generate predictions for masked gender tokens in the sample contexts provided.

```bash
python predict_mask.py
```

Each output file is formatted as an array of entries: (`tokens_tensor`, `segments_tensor`, `tokenized_text`, (`gender_index`, `query_index`, `gender_word`, `query_word`), `norm_prob`, `top5_preds`). All items in each entry are identical to those in the input file, except for the last two items.
- `norm_prob`: normalized probability of the correct gender prediction (e.g. if the correct gender is female, `norm_prob` would return `pr(female) / (pr(female) + pr(male))`
- `top5_preds`: top 5 predictions for the masked gender token

To perform further analysis on the results, you can use the `read_context_windows()` function to read the data in the output files generated by `predict_mask.py`.

## Misc

You can play around with the `feed_sample_sentence()` function in `predict_mask.py` to feed a custom sample input into the fine-tuned BERT model and output the top 5 predictions.